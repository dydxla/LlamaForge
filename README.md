# LlamaForge ğŸ¦™

<div align="center">

![Python Version](https://img.shields.io/badge/python-3.10%2B-blue)
![License](https://img.shields.io/badge/license-Apache%202.0-green)
![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20Mac-blue)

</div>

LlamaForgeëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì‰½ê²Œ íŒŒì¸íŠœë‹í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. Llama, DeepSeek, Grok ë“± Hugging Faceì—ì„œ ì§€ì›í•˜ëŠ” ë‹¤ì–‘í•œ LLM ëª¨ë¸ì„ LoRAë¥¼ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì¸íŠœë‹í•  ìˆ˜ ìˆìœ¼ë©°, Windows, Linux, Mac í™˜ê²½ ëª¨ë‘ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

- ğŸš€ ê°„ë‹¨í•œ APIë¡œ ë‹¤ì–‘í•œ LLM ëª¨ë¸ íŒŒì¸íŠœë‹ (Llama, DeepSeek, Grok ë“±)
- ğŸ”§ LoRAë¥¼ í†µí•œ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ ì§€ì›
- ğŸ“Š BoolQ, SQuAD ë“± í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ë¥¼ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
- ğŸŒ Windows, Linux, Mac í™˜ê²½ ì§€ì›
- ğŸ› ï¸ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ ê°€ëŠ¥í•œ í•™ìŠµ ì„¤ì •
- ğŸ“Š ê¸°ë³¸ ì œê³µë˜ëŠ” ë°ì´í„°ì…‹ í…œí”Œë¦¿

## ğŸ“‹ ìš”êµ¬ì‚¬í•­

- Python 3.10 ì´ìƒ
- CUDA ì§€ì› GPU (ê¶Œì¥)
- Hugging Face ê³„ì • ë° í† í°
- ì‚¬ìš©í•˜ë ¤ëŠ” ëª¨ë¸ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œ (ì˜ˆ: Llama ëª¨ë¸ì˜ ê²½ìš° Meta AI ìŠ¹ì¸ í•„ìš”)

## ğŸš€ ì„¤ì¹˜ ë°©ë²•

1. ì €ì¥ì†Œ í´ë¡ :
```bash
git clone https://github.com/dydxla/LlamaForge.git
cd LlamaForge
```

2. íŒ¨í‚¤ì§€ ì„¤ì¹˜:
```bash
pip install -e .
```

## ğŸ”‘ Hugging Face ì„¤ì •

1. [Hugging Face](https://huggingface.co/) ê³„ì • ìƒì„±
2. Access Token ìƒì„±:
   - Settings â†’ Access Tokens â†’ New token
   - í† í° ìƒì„± ì‹œ read ê¶Œí•œ ë¶€ì—¬
3. ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ ì–»ê¸°:
   - í•„ìš”í•œ ê²½ìš° í•´ë‹¹ ëª¨ë¸ì˜ ë¼ì´ì„ ìŠ¤ ë™ì˜ (ì˜ˆ: Llama ëª¨ë¸ì˜ ê²½ìš° [Meta AI](https://ai.meta.com/llama/)ì—ì„œ ì‹ ì²­)
   - Hugging Faceì—ì„œ í•´ë‹¹ ëª¨ë¸ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œ ìš”ì²­

## ğŸ’» ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from llamaforge.finetune import FinetuneTrainer

# Hugging Face í† í° ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í–ˆê±°ë‚˜ ì§ì ‘ ì „ë‹¬)
trainer = FinetuneTrainer(
    model_name="meta-llama/Llama-2-7b-chat-hf",  # ë˜ëŠ” ë‹¤ë¥¸ LLM ëª¨ë¸ (ì˜ˆ: deepseek-ai/deepseek-llm-7b-base)
    dataset_path="your_dataset_path",  # ë°ì´í„°ì…‹ ê²½ë¡œ
    hf_token="your_huggingface_token"  # ì„ íƒì‚¬í•­: í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í–ˆë‹¤ë©´ ìƒëµ ê°€ëŠ¥
)

# íŒŒì¸íŠœë‹ ì‹œì‘
trainer.run_finetune()
```

### í™˜ê²½ ë³€ìˆ˜ë¡œ í† í° ì„¤ì •

Windows PowerShell:
```powershell
$env:HUGGING_FACE_HUB_TOKEN = "your_token"
```

Linux/Mac:
```bash
export HUGGING_FACE_HUB_TOKEN="your_token"
```

### ì»¤ìŠ¤í…€ ì„¤ì •

```python
from llamaforge.finetune import FinetuneTrainer

trainer = FinetuneTrainer(
    model_name="deepseek-ai/deepseek-llm-7b-base",  # ì˜ˆ: DeepSeek ëª¨ë¸
    dataset_path="your_dataset_path",
    model_dtype=torch.float16,  # ëª¨ë¸ ë°ì´í„° íƒ€ì…
    template_type="chatbot",    # ë°ì´í„°ì…‹ í…œí”Œë¦¿ íƒ€ì…
    initial_configs={           # í•™ìŠµ ì„¤ì •
        "output_dir": "./output",
        "num_train_epochs": 3,
        "per_device_train_batch_size": 4,
        "gradient_accumulation_steps": 4,
        "learning_rate": 2e-4,
        "fp16": True,
    },
    initial_lora_configs={      # LoRA ì„¤ì •
        "r": 8,
        "lora_alpha": 32,
        "target_modules": ["q_proj", "v_proj"],  # ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
    }
)

# íŒŒì¸íŠœë‹ ì‹œì‘
trainer.run_finetune()
```

### ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

```python
from llamaforge.eval.benchmarks import BenchmarkEvaluator

# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í‰ê°€ê¸° ìƒì„±
evaluator = BenchmarkEvaluator(
    model_path="path/to/your/model",      # í‰ê°€í•  ëª¨ë¸ ê²½ë¡œ
    tokenizer_path="path/to/tokenizer"    # í† í¬ë‚˜ì´ì € ê²½ë¡œ (ì„ íƒì‚¬í•­)
)

# ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (BoolQ)
boolq_result = evaluator.run_benchmarks("boolq")
print(f"BoolQ Accuracy: {boolq_result['boolq']:.2f}%")

# ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (BoolQ + SQuAD)
results = evaluator.run_benchmarks(["boolq", "squad"])
print(f"BoolQ Accuracy: {results['boolq']:.2f}%")
em, f1 = results['squad']
print(f"SQuAD - Exact Match: {em:.2f}%, F1 Score: {f1:.2f}%")

# ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
all_results = evaluator.run_benchmarks()
```

í‰ê°€ ì„¤ì •ì€ `src/llamaforge/eval/configs/config.yaml`ì—ì„œ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
```yaml
# í‰ê°€í•˜ê³ ì í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬
benchmarks:
    - boolq
    - squad

# í‰ê°€ ì§€í‘œ ì„¤ì •
metrics:
    - exact_match
    - f1

# ì‹¤í–‰ ì˜µì…˜
device: cuda      # ì‹¤í–‰ ì¥ì¹˜ ("cuda", "cpu")
max_new_tokens: 100     # ëª¨ë¸ ì¶œë ¥ì˜ ìµœëŒ€ ê¸¸ì´
batch_size: 4       # ë°°ì¹˜ í¬ê¸°
```

## ğŸ“š ë°ì´í„°ì…‹ í˜•ì‹

ë°ì´í„°ì…‹ì€ ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ì„ ë”°ë¦…ë‹ˆë‹¤:

```json
[
    {
        "instruction": "ì§ˆë¬¸ ë˜ëŠ” ì§€ì‹œì‚¬í•­",
        "output": "ì‘ë‹µ ë˜ëŠ” ì¶œë ¥"
    },
    ...
]
```

ë°ì´í„°ì…‹ì€ ë‚´ë¶€ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ OpenAI ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤:
```json
{
    "messages": [
        {
            "role": "system",
            "content": "You are the AI assistant created to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects."
        },
        {
            "role": "user",
            "content": "ì§ˆë¬¸ ë˜ëŠ” ì§€ì‹œì‚¬í•­"
        },
        {
            "role": "assistant",
            "content": "ì‘ë‹µ ë˜ëŠ” ì¶œë ¥"
        }
    ]
}
```

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

í”„ë¡œì íŠ¸ì— ê¸°ì—¬í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´:

1. ì´ ì €ì¥ì†Œë¥¼ í¬í¬í•©ë‹ˆë‹¤
2. ìƒˆë¡œìš´ ë¸Œëœì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (`git checkout -b feature/amazing-feature`)
3. ë³€ê²½ì‚¬í•­ì„ ì»¤ë°‹í•©ë‹ˆë‹¤ (`git commit -m 'Add amazing feature'`)
4. ë¸Œëœì¹˜ì— í‘¸ì‹œí•©ë‹ˆë‹¤ (`git push origin feature/amazing-feature`)
5. Pull Requestë¥¼ ìƒì„±í•©ë‹ˆë‹¤

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” Apache License 2.0 í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ‘¥ ì‘ì„±ì

- dydxla - [GitHub](https://github.com/dydxla)

## ğŸ™ ê°ì‚¬ì˜ ê¸€

- [Meta AI](https://ai.meta.com/) - Llama ëª¨ë¸ ì œê³µ
- [DeepSeek AI](https://deepseek.ai/) - DeepSeek ëª¨ë¸ ì œê³µ
- [Hugging Face](https://huggingface.co/) - í›Œë¥­í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ì œê³µ
- [PEFT](https://github.com/huggingface/peft) - LoRA êµ¬í˜„
- [TRL](https://github.com/huggingface/trl) - SFT Trainer êµ¬í˜„